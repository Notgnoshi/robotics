{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "Austin Gill\n",
    "\n",
    "* 11: 3\n",
    "* 16: 1, 2\n",
    "* 17: 2, 3, 4\n",
    "* 18: 1.1\n",
    "* Text Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config InlineBackend.figure_format = 'retina'\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "import itertools\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "# Apparently, SNS stands for \"Samuel Norman Seaborn\", a fictional\n",
    "# character from The West Wing\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11.3\n",
    "\n",
    "Write a Python program to navigate a robot using the Wavefront algorithm. Demonstrate on a map with multiple obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Creation with Celular Automata\n",
    "\n",
    "I couldn't figure out how to make an interesting map by hand in a way that didn't suck, so after a little research, here's how I can create a map with celular automata. The algorithm is from [here](http://brogue.wikia.com/wiki/Level_Generation#Lakes) and modified to fit my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a nice way to plot a map and an optional path to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(a, path=None, **kwargs):\n",
    "    \"\"\"Plot a given map.\"\"\"\n",
    "    sns.heatmap(a,\n",
    "                cbar=False,\n",
    "                square=True,\n",
    "                xticklabels=False, yticklabels=False,\n",
    "                **kwargs)\n",
    "    if path is not None:\n",
    "        sns.heatmap(a,\n",
    "                    mask=path,\n",
    "                    cmap=mpl.colors.ListedColormap(['m']),\n",
    "                    cbar=False,\n",
    "                    square=True,\n",
    "                    xticklabels=False, yticklabels=False,\n",
    "                    **kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate a seed matrix randomly filled with the specified percentage of obstacle values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seed(rows, cols, percentage):\n",
    "    \"\"\"Generate a seed of the given size with the given percentage\n",
    "    of filled cells.\n",
    "    \"\"\"\n",
    "    # Create a 1D array and then reshape it because I'm a scrub.\n",
    "    a = np.zeros(rows * cols, dtype=int)\n",
    "    # Set the given percentage of the values to be filled.\n",
    "    a[:int(percentage * rows * cols)] = -1\n",
    "    np.random.shuffle(a)\n",
    "    # Return a shuffled 2D view of the array.\n",
    "    return np.reshape(a, (rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(a, i, j):\n",
    "    \"\"\"Get the 3x3 submatrix around coordinate (i, j)\"\"\"\n",
    "    xmin = i - 1 if i - 1 >= 0 else 0\n",
    "    ymin = j - 1 if j - 1 >= 0 else 0\n",
    "    # Add 2 because the end is exclusive.\n",
    "    return a[xmin:i+2, ymin:j+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we glob together cells matching certain conditions. An empty cell is populated if it is surrounded by 6 or more filled cells. A filled cell remains filled if it is surrounded by 4 or more filled cells. Any other cells die or remain dead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob(a):\n",
    "    \"\"\"Run the blob algorithm once on the given matrix.\"\"\"\n",
    "    rows, cols = a.shape\n",
    "    for i, j in itertools.product(range(rows), range(cols)):\n",
    "        N = neighbors(a, i, j)\n",
    "        # Cell is born if surrounded by more than 5 neighbors.\n",
    "        if not a[i, j] and -np.sum(N) > 5:\n",
    "            a[i, j] = -1\n",
    "        # Cell dies if surrounded by 3 or fewer neighbors.\n",
    "        elif a[i, j] and -np.sum(N) <= 3:\n",
    "            a[i, j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So generate the seed and run two iterations to \"smooth out\" the obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_seed(15, 15, 0.45)\n",
    "plot(a)\n",
    "\n",
    "blob(a)\n",
    "blob(a)\n",
    "plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wavefront Algorithm\n",
    "\n",
    "The wavefront algorithm works in two steps\n",
    "\n",
    "1. Fill the free space starting with the goal.\n",
    "\n",
    "   This seems like it would work best recursively, because you need to start with the goal and work your way outward. It needs to be a breadth first recursive solution because you want to work outwards relatively evenly.\n",
    "\n",
    "2. From the start point, randomly pick from the lowest labeled cells (there could be more than one with the same distance).\n",
    "\n",
    "I will use 4-way travel rather than 8-way (no diagonal moves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(a, coord):\n",
    "    \"\"\"Determines if the given coordinate is valid for the given array.\"\"\"\n",
    "    coord = np.array(coord)\n",
    "    return np.all(np.zeros_like(a.shape) <= coord) and np.all(coord < a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a breadth first flood fill algorithm because we want to mark all of a cell's immediate neighbors before moving on to their neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flood_fill(a, start):\n",
    "    \"\"\"Breadth-first flood fill the given map from the given start point.\"\"\"\n",
    "    q = deque()\n",
    "    # It's a deque, so append and pop from opposite ends -_-\n",
    "    q.appendleft(start)\n",
    "\n",
    "    while len(q) > 0:\n",
    "        i, j = q.pop()\n",
    "        adjacents = [(i, j-1), (i+1, j), (i, j+1), (i-1, j)]\n",
    "\n",
    "        distance = a[i, j]\n",
    "        distance += 1\n",
    "\n",
    "        # Consider only cells inside the given matrix.\n",
    "        for cell in filter(functools.partial(valid, a), adjacents):\n",
    "            # Update any cells that are not the start and that haven't\n",
    "            # been updated yet.\n",
    "            if cell != start and a[cell] == 0:\n",
    "                a[cell] = distance\n",
    "                q.appendleft(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we flood fill and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_fill(a, (0, 0))\n",
    "plot(a, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with this implementation, the last encountered cell with the minimal value will be chosen. With the ordering of the `adjacents` list in the `descent()` function, this corresponds to prefering vertical paths over horizontal paths of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_index(a, indices):\n",
    "    \"\"\"Get the index with the minimum value from a list of indices.\"\"\"\n",
    "    idx = indices[0]\n",
    "    val = a[idx]\n",
    "\n",
    "    for cell in indices:\n",
    "        if a[cell] <= val:\n",
    "            idx = cell\n",
    "            val = a[idx]\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second phase of the wavefront algorithm is to descend from the goal to the start point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(a, goal):\n",
    "    \"\"\"Get a path through the map descending from the given goal point.\"\"\"\n",
    "    yield goal\n",
    "\n",
    "    while True:\n",
    "        i, j = goal\n",
    "        adjacents = [(i, j-1), (i+1, j), (i, j+1), (i-1, j), (i, j)]\n",
    "        # Filter out cells outside the map.\n",
    "        adjacents = [p for p in adjacents if valid(a, p)]\n",
    "        # Filter out obstacle cells.\n",
    "        adjacents = [p for p in adjacents if a[p] >= 0]\n",
    "\n",
    "        # If there's nowhere to go, we've reached the end\n",
    "        if not adjacents:\n",
    "            break\n",
    "\n",
    "        idx = min_index(a, adjacents)\n",
    "\n",
    "        # If we don't make progress or we've reached the start, stop.\n",
    "        if idx == goal:\n",
    "            break\n",
    "            \n",
    "        yield idx\n",
    "        \n",
    "        if a[idx] == 0:\n",
    "            break\n",
    "\n",
    "        goal = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the resulting path from $(14, 14)$ to the start point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark out the path with a boolean mask.\n",
    "mask = np.ones_like(a, dtype=bool)\n",
    "\n",
    "for cell in descent(a, (14, 14)):\n",
    "    mask[cell] = False\n",
    "\n",
    "# Plot the filled map and the path.\n",
    "plot(a, path=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what happens if there is no path from the start to the goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = generate_seed(15, 15, 0.55)\n",
    "plot(b)\n",
    "\n",
    "blob(b)\n",
    "blob(b)\n",
    "plot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_fill(b, (0, 0))\n",
    "plot(b, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark out the path with a boolean mask.\n",
    "mask = np.ones_like(b, dtype=bool)\n",
    "\n",
    "for cell in descent(b, (14, 14)):\n",
    "    mask[cell] = False\n",
    "\n",
    "# Plot the filled map and the path.\n",
    "plot(b, path=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the flood fill algorithm leaves all unreachable cells with distance `0`, so when we begin our descent, we immediately believe we have found the start point, so we do not progress. This is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 16.1\n",
    "\n",
    "Let $z_1=284$, $z_2=257$, $z_3=295$, be measurements from sensors which have normally distributed errors with standard deviations $\\sigma_1=10$, $\\sigma_2=20$, $\\sigma_3=15$, respectively. What is the best estimate for the measured state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to weight the better measurments more, so we use\n",
    "\n",
    "$$\\hat x = \\frac{1}{S} \\sum_i s_i z_i$$\n",
    "\n",
    "where $S = \\sum_i s_i$ and $s_i = \\frac{1}{\\sigma_1^2}$. I'm a scrub, so here's some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([284, 257, 295])\n",
    "sigma = np.array([10, 20, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1 / sigma**2\n",
    "x = (1 / np.sum(s)) * np.sum(s * z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimate makes sense, because we would weight $z_1$ more than the rest, so that's the measurement the estimate is closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 16.2\n",
    "\n",
    "You are given three distance sensors which all measure the same distance. To determine the accuracy of each you run repeated measurements of an object which you setup so that the sensor will return 2 meters. Running 40 measurements you obtain the data listed below. Next you measure the distance of an object ahead of the robot. Sensor $A$ gives 2.4577696, sensor $B$ gives 1.8967743, and sensor $C$ gives 2.1352561. Combine these readings to make a more accurate estimate of the object distance. Hint: you need to figure out the distributions and the data according to the variances.\n",
    "<!--\n",
    "|    $A$    |    $B$    |    $C$    |\n",
    "|:---------:|:---------:|:---------:|\n",
    "| 2.2411549 | 1.8286673 | 2.3295015 |\n",
    "| 2.3366108 | 1.9243295 | 2.4867167 |\n",
    "| 1.9687234 | 1.8972737 | 2.5489412 |\n",
    "| 2.1240351 | 2.0961534 | 1.9834876 |\n",
    "| 2.3984044 | 1.7985819 | 2.6153805 |\n",
    "| 2.3523899 | 1.8377782 | 1.8132444 |\n",
    "| 2.1074266 | 1.8358201 | 2.5563951 |\n",
    "| 2.4711542 | 1.8875839 | 2.2031291 |\n",
    "| 2.1998000 | 1.8116113 | 2.3117542 |\n",
    "| 2.2710086 | 1.8701890 | 2.3495262 |\n",
    "| 2.3530473 | 1.7646824 | 2.0109293 |\n",
    "| 2.4391559 | 1.9499153 | 2.5030771 |\n",
    "| 2.2066306 | 1.9243432 | 2.3561112 |\n",
    "| 2.3000099 | 1.8309696 | 2.3097754 |\n",
    "| 2.2235766 | 1.8453219 | 2.2940692 |\n",
    "| 2.1396901 | 1.8390955 | 2.1904604 |\n",
    "| 2.0929719 | 1.7978329 | 2.5693897 |\n",
    "| 2.3154159 | 1.8217245 | 1.9332188 |\n",
    "| 2.3716302 | 1.9558670 | 2.3002433 |\n",
    "| 2.2611420 | 1.8654487 | 2.5508342 |\n",
    "| 2.1415088 | 1.7836290 | 2.6884786 |\n",
    "| 2.2088487 | 1.9245743 | 2.5037028 |\n",
    "| 2.2714614 | 1.8918415 | 2.7112663 |\n",
    "| 2.3345816 | 1.8275421 | 2.1656644 |\n",
    "| 2.3052296 | 1.8494488 | 2.1940472 |\n",
    "| 2.1600041 | 1.7632971 | 2.2703708 |\n",
    "| 2.0630943 | 1.8396972 | 2.6488544 |\n",
    "| 2.0997821 | 1.8412331 | 2.1828831 |\n",
    "| 2.3037175 | 1.7761007 | 2.2959535 |\n",
    "| 2.4536524 | 1.8542271 | 2.0446945 |\n",
    "| 2.3909478 | 1.8649815 | 2.7852822 |\n",
    "| 2.1195966 | 1.9533324 | 2.5700007 |\n",
    "| 2.0205112 | 1.8857815 | 2.1113650 |\n",
    "| 2.1708006 | 1.7115595 | 2.1215336 |\n",
    "| 2.0800748 | 1.9403332 | 2.3126032 |\n",
    "| 2.3332722 | 1.8530670 | 2.4687277 |\n",
    "| 2.0826115 | 1.8279041 | 2.6104026 |\n",
    "| 2.2652480 | 1.9058054 | 2.3165716 |\n",
    "| 2.3734464 | 1.9632258 | 2.0907554 |\n",
    "| 2.0563260 | 1.9367908 | 2.2130578 | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "za = np.array([\n",
    "    2.2411549, 2.3366108, 1.9687234, 2.1240351, 2.3984044, 2.3523899, 2.1074266,\n",
    "    2.4711542, 2.1998000, 2.2710086, 2.3530473, 2.4391559, 2.2066306, 2.3000099,\n",
    "    2.2235766, 2.1396901, 2.0929719, 2.3154159, 2.3716302, 2.2611420, 2.1415088,\n",
    "    2.2088487, 2.2714614, 2.3345816, 2.3052296, 2.1600041, 2.0630943, 2.0997821,\n",
    "    2.3037175, 2.4536524, 2.3909478, 2.1195966, 2.0205112, 2.1708006, 2.0800748,\n",
    "    2.3332722, 2.0826115, 2.2652480, 2.3734464, 2.0563260,\n",
    "])\n",
    "\n",
    "zb = np.array([\n",
    "    1.8286673, 1.9243295, 1.8972737, 2.0961534, 1.7985819, 1.8377782, 1.8358201,\n",
    "    1.8875839, 1.8116113, 1.8701890, 1.7646824, 1.9499153, 1.9243432, 1.8309696,\n",
    "    1.8453219, 1.8390955, 1.7978329, 1.8217245, 1.9558670, 1.8654487, 1.7836290,\n",
    "    1.9245743, 1.8918415, 1.8275421, 1.8494488, 1.7632971, 1.8396972, 1.8412331,\n",
    "    1.7761007, 1.8542271, 1.8649815, 1.9533324, 1.8857815, 1.7115595, 1.9403332,\n",
    "    1.8530670, 1.8279041, 1.9058054, 1.9632258, 1.9367908,\n",
    "])\n",
    "\n",
    "zc = np.array([\n",
    "    2.3295015, 2.4867167, 2.5489412, 1.9834876, 2.6153805, 1.8132444, 2.5563951,\n",
    "    2.2031291, 2.3117542, 2.3495262, 2.0109293, 2.5030771, 2.3561112, 2.3097754,\n",
    "    2.2940692, 2.1904604, 2.5693897, 1.9332188, 2.3002433, 2.5508342, 2.6884786,\n",
    "    2.5037028, 2.7112663, 2.1656644, 2.1940472, 2.2703708, 2.6488544, 2.1828831,\n",
    "    2.2959535, 2.0446945, 2.7852822, 2.5700007, 2.1113650, 2.1215336, 2.3126032,\n",
    "    2.4687277, 2.6104026, 2.3165716, 2.0907554, 2.2130578,\n",
    "])\n",
    "\n",
    "za.sort()\n",
    "zb.sort()\n",
    "zc.sort()\n",
    "\n",
    "data = np.array([za, zb, zc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(za, sp.stats.norm.pdf(za, za.mean(), za.std()), label='Sensor $A$')\n",
    "plt.plot(zb, sp.stats.norm.pdf(zb, zb.mean(), zb.std()), label='Sensor $B$')\n",
    "plt.plot(zc, sp.stats.norm.pdf(zc, zc.mean(), zc.std()), label='Sensor $C$')\n",
    "\n",
    "plt.title('Sensor Normal Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the distributions around the true value\n",
    "shifts = 2 - np.array([za.mean(), zb.mean(), zc.mean()])\n",
    "stds = np.array([za.std(), zb.std(), zc.std()])\n",
    "\n",
    "za += shifts[0]\n",
    "zb += shifts[1]\n",
    "zc += shifts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(za, sp.stats.norm.pdf(za, za.mean(), za.std()), label='Sensor $A$')\n",
    "plt.plot(zb, sp.stats.norm.pdf(zb, zb.mean(), zb.std()), label='Sensor $B$')\n",
    "plt.plot(zc, sp.stats.norm.pdf(zc, zc.mean(), zc.std()), label='Sensor $C$')\n",
    "\n",
    "plt.title('Centered Normal Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimally fuse these measurements with the formula\n",
    "\n",
    "$$\\hat x = \\frac{\\sum_i \\frac{z_i}{\\sigma_i^2}}{\\sum_i \\frac{1}{\\sigma_i^2}}$$\n",
    "\n",
    "which will have a variance\n",
    "\n",
    "$$\\hat{\\sigma^2} = \\left(\\sum_i \\frac{1}{\\sigma_i^2}\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readings after experiment\n",
    "z = np.array([2.4577696, 1.8967743, 2.1352561])\n",
    "corrected = z + shifts\n",
    "print('correctted measurements:', corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = np.sum(corrected / stds**2) / np.sum(1 / stds**2)\n",
    "variance = 1 / np.sum(1 / stds**2)\n",
    "\n",
    "print('estimate:', xhat)\n",
    "print('variance:', variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 17.2\n",
    "\n",
    "Assume that one has three different measurements for the location of some object. The three measurements with the covariances are\n",
    "\n",
    "$$\\begin{align}\n",
    "    (10.5, 18.2)&, \\quad\\begin{pmatrix}\n",
    "        0.1 & 0.01 \\\\\n",
    "        0.01 & 0.15\n",
    "    \\end{pmatrix} \\\\\n",
    "    (10.75, 18.0)&, \\quad\\begin{pmatrix}\n",
    "        0.05 & 0.005 \\\\\n",
    "        0.005 & 0.05\n",
    "    \\end{pmatrix} \\\\\n",
    "    (9.9, 19.1)&, \\quad\\begin{pmatrix}\n",
    "        0.2 & 0.05 \\\\\n",
    "        0.05 & 0.25\n",
    "    \\end{pmatrix}\n",
    "\\end{align}$$\n",
    "Fuse this data into one measurement and provide an estimate of the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.array([10.5, 18.2])\n",
    "V1 = np.array([[0.1,  0.01],\n",
    "               [0.01, 0.15]])\n",
    "\n",
    "z2 = np.array([10.75, 18.0])\n",
    "V2 = np.array([[0.05, 0.005],\n",
    "               [0.005, 0.05]])\n",
    "\n",
    "z3 = np.array([9.9, 19.1])\n",
    "V3 = np.array([[0.2,  0.05],\n",
    "               [0.05, 0.25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([z1, z2, z3])\n",
    "V = np.array([V1, V2, V3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to save each estimate and covariance matrix in.\n",
    "xhats = np.zeros_like(z)\n",
    "Ps = np.zeros_like(V)\n",
    "\n",
    "xhats[0] = z[0]\n",
    "Ps[0] = V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    K = Ps[i - 1] @ sp.linalg.inv(Ps[i - 1] + V[i])\n",
    "    xhats[i] = xhats[i - 1] + K @ (z[i] - xhats[i - 1])\n",
    "    Ps[i] = Ps[i - 1] - K @ Ps[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xhats)\n",
    "print(Ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigsorted(cov):\n",
    "    \"\"\"Get the sorted eigenvalues of a 2x2 covariance matrix\"\"\"\n",
    "    vals, vecs = sp.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    return vals[order], vecs[:, order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ellipse(cov, center, nstd=1, ax=None, **kwargs):\n",
    "    \"\"\"Plot an error ellipse.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    \n",
    "    width, height = nstd * np.sqrt(vals)\n",
    "    ellipse = Ellipse(xy=center, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "    return ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (xhat, P) in enumerate(zip(xhats, Ps)):\n",
    "    x, y = xhat\n",
    "    plt.plot(x, y, '.', label=f'$\\hat x_{i}$')\n",
    "    plot_ellipse(P, xhat, alpha=0.2)\n",
    "\n",
    "plt.title('$\\hat x$ Estimates and Error Ellipses')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem relies on assuming that the initial estimate $\\hat x_0$ is $z_1$. My first though was to make a wild initial guess and see what happened. Here's the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([10, 18])\n",
    "P0 = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "z = np.array([x0, z1, z2, z3])\n",
    "V = np.array([P0, V1, V2, V3])\n",
    "\n",
    "xhats = np.zeros_like(z)\n",
    "Ps = np.zeros_like(V)\n",
    "\n",
    "xhats[0] = z[0]\n",
    "Ps[0] = V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    K = Ps[i - 1] @ sp.linalg.inv(Ps[i - 1] + V[i])\n",
    "    xhats[i] = xhats[i - 1] + K @ (z[i] - xhats[i - 1])\n",
    "    Ps[i] = Ps[i - 1] - K @ Ps[i - 1]\n",
    "\n",
    "print(xhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (xhat, P) in enumerate(zip(xhats, Ps)):\n",
    "    x, y = xhat\n",
    "    plt.plot(x, y, '.', label=f'$\\hat x_{i}$')\n",
    "    plot_ellipse(P, xhat, alpha=0.2)\n",
    "\n",
    "plt.title('$\\hat x$ Estimates and Error Ellipses')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the only thing this shows is that the error ellipses get better and better with each measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 17.3\n",
    "\n",
    "Run a simulation on\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\dot x &= y \\\\\n",
    "    \\dot y &= -\\cos x + 0.5 \\sin t\n",
    "\\end{align*}$$\n",
    "\n",
    "adding noise to the $x$ and $y$ components (with variance = $0.2$ on each). Let $\\Delta t=0.1$. Assume that you can observe the first variable, $x$, with variance $0.25$. Record the observations. Write a program to run the EKF on the observed data and compare the state estimate to the original values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we discretize\n",
    "\n",
    "$$\\frac{x_{n + 1} - x_n}{\\Delta t} = y_n$$\n",
    "$$\\frac{y_{n + 1} - y_n}{\\Delta t} = -\\cos x_n + 0.5 \\sin t_n$$\n",
    "\n",
    "and get\n",
    "\n",
    "$$x_{n + 1} = \\Delta t y_n + x_n$$\n",
    "$$y_{n + 1} = \\Delta t ( -\\cos x_n + 0.5 \\sin t_n ) + y_n$$\n",
    "\n",
    "Then we linearize and get\n",
    "\n",
    "$$F_k = \\begin{pmatrix}\n",
    "1 & \\Delta t \\\\\n",
    "\\Delta t \\sin x_n & 1 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Since we observe only the first variable, $H = \\begin{pmatrix}1 & 0\\end{pmatrix}$.\n",
    "\n",
    "We have process variance\n",
    "\n",
    "$$V = \\begin{pmatrix}\n",
    "0.2 & 0 \\\\\n",
    "0 & 0.2 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "and observation variance\n",
    "\n",
    "$$W = \\begin{pmatrix}\n",
    "0.25\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "which is a singleton matrix because we are observing only a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "process_variance = 0.2\n",
    "observation_variance = 0.25\n",
    "V = np.array([[process_variance, 0],\n",
    "              [0, process_variance]])\n",
    "W = np.array([[observation_variance]])\n",
    "H = np.array([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(state):\n",
    "    \"\"\"The physics matrix at the given state.\"\"\"\n",
    "    assert state.shape == (2, 1)\n",
    "    x, _ = state\n",
    "    return np.array([[1, dt],\n",
    "                     [dt * np.sin(x), 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(state, time):\n",
    "    \"\"\"The discretized motion model.\"\"\"\n",
    "    assert state.shape == (2, 1)\n",
    "    x, y = state\n",
    "    return np.array([dt * y + x,\n",
    "                     dt * (-np.cos(x) + 0.5 * np.sin(time)) + y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(x0):\n",
    "    \"\"\"A simulation generator for the given physics matrix\n",
    "    and initial position. Yields (state, observation) tuples.\n",
    "    \"\"\"\n",
    "    prev_state = x0\n",
    "    while True:\n",
    "        process_noise = np.random.multivariate_normal([0, 0], V, 1).T\n",
    "        assert process_noise.shape == (2, 1)\n",
    "        observation_noise = np.random.multivariate_normal([0], W, 1).T\n",
    "        assert observation_noise.shape == (1, 1)\n",
    "        state = F(prev_state) @ prev_state + process_noise\n",
    "        assert state.shape == (2, 1)\n",
    "        observation = state + observation_noise\n",
    "        assert observation.shape == (2, 1)\n",
    "        \n",
    "        # Even though we're only observing one variable, yield both and\n",
    "        # apply the observation matrix to hide the second.\n",
    "        yield state, observation\n",
    "        \n",
    "        prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x0, P0, t0):\n",
    "    \"\"\"Run the Extended Kalman Filter starting with the given\n",
    "    initial state, initial covariance matrix, and physics matrix.\n",
    "    \"\"\"\n",
    "    time = t0\n",
    "    state_estimate = x0\n",
    "    state_covariance = P0\n",
    "\n",
    "    for state, observation in simulation(x0):\n",
    "        assert state.shape == (2, 1)\n",
    "        assert observation.shape == (2, 1)\n",
    "        prediction = f(state_estimate, time)\n",
    "        assert prediction.shape == (2, 1)\n",
    "        pred_covar = F(state_estimate) @ state_covariance @ F(state_estimate).T + V\n",
    "        assert pred_covar.shape == (2, 2)\n",
    "        kalman_gain = pred_covar @ H.T @ sp.linalg.inv(H @ pred_covar @ H.T + W)\n",
    "        assert kalman_gain.shape == (2, 1)\n",
    "        # The simulation simulates both variables, but we only observe the first.\n",
    "        residual = H @ observation - H @ prediction\n",
    "        assert residual.shape == (1, 1)\n",
    "        state_estimate = prediction + kalman_gain @ residual\n",
    "        assert state_estimate.shape == (2, 1)\n",
    "        state_covariance = (np.eye(2) - kalman_gain @ H) @ pred_covar\n",
    "        assert state_covariance.shape == (2, 2)\n",
    "        \n",
    "        yield state, observation, state_estimate, state_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[0, 0]]).T\n",
    "P0 = np.array([[1, 0],\n",
    "               [0, 1]])\n",
    "t0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "ekf = filter(x0, P0, t0)\n",
    "# Run N iterations of the EKF and save the results\n",
    "vals = [next(ekf) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, observations, estimates, covariances = zip(*vals)\n",
    "\n",
    "x, y = zip(*states)\n",
    "zx, zy = zip(*observations)\n",
    "xhat, yhat = zip(*estimates)\n",
    "\n",
    "# zip returns tuples. Convert to properly dimensioned numpy arrays for math.\n",
    "yhat = np.array(yhat).reshape(N)\n",
    "xhat = np.array(xhat).reshape(N)\n",
    "\n",
    "# Ignore the cross variances because I'm not sure how to plot that...\n",
    "xstd = np.array([np.sqrt(cov[0, 0]) for cov in covariances])\n",
    "ystd = np.array([np.sqrt(cov[1, 1]) for cov in covariances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N), x, label='$x$')\n",
    "plt.plot(range(N), zx, '.', label='$z_x$')\n",
    "plt.plot(range(N), xhat, label=r'$\\hat x$', color='r')\n",
    "\n",
    "plt.title('EKF $x$ results')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(N), y, label='$y$')\n",
    "# Don't plot the observations because we didn't observe this variable.\n",
    "# plt.plot(range(N), zy, '.', label='$z_y$')\n",
    "plt.plot(range(N), yhat, label=r'$\\hat y$', color='r')\n",
    "\n",
    "plt.title('EKF $y$ (hidden variable) results')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N), x, label='$x$')\n",
    "# plt.plot(range(N), zx, '.', label='$z_x$')\n",
    "plt.plot(range(N), xhat, label=r'$\\hat x$', color='r')\n",
    "plt.fill_between(range(N), xhat - xstd, xhat + xstd, alpha=0.3, label=r'$1$ std dev', color='r')\n",
    "\n",
    "plt.title('EKF $x$ results')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(N), y, label='$y$')\n",
    "plt.plot(range(N), yhat, label=r'$\\hat y$', color='r')\n",
    "plt.fill_between(range(N), yhat - ystd, yhat + ystd, alpha=0.3, label=r'$1$ std dev', color='r')\n",
    "\n",
    "plt.title('EKF $y$ (hidden variable) results')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 17.4\n",
    "\n",
    "Differential Drive - EKF. The motion equations for a differential drive robot are given below. Assume that the wheels are $5$cm in radius and the wheelbase is $12$cm.\n",
    "\n",
    "Use $\\Delta t = 0.2$ and discretize. Assume the covariance of the state transition is $V$. Assume that you have a local GPS system that gives $(x,y)$ data subject to Gaussian noise with covariance $W$. The units on the noise are given in cm.\n",
    "\n",
    "1. Starting at $t=0$, $x=0$, $y=0$, $\\theta=0$, predict location when wheel velocities are:\n",
    "   \n",
    "   $$\\begin{align*}\n",
    "     0  \\leq t \\leq 5 &,\\quad  \\omega_1 =  \\omega_2 = 3 \\\\\n",
    "     5  \\leq t \\leq 6 &,\\quad  \\omega_1 = -\\omega_2 = 1 \\\\\n",
    "     6  \\leq t \\leq 10&,\\quad  \\omega_1 =  \\omega_2 = 3 \\\\\n",
    "     10 \\leq t \\leq 11&,\\quad -\\omega_1 =  \\omega_2 = 1 \\\\\n",
    "     11 \\leq t \\leq 16&,\\quad  \\omega_1 =  \\omega_2 = 3 \\\\\n",
    "   \\end{align*}$$\n",
    "   assuming that you have Gaussian noise on the process with covariance.\n",
    "   $$V = \\begin{pmatrix}.05 &  .02 & 0.01\\\\.02& .05& 0.01\\\\ 0.01& 0.01& .1\\end{pmatrix}$$\n",
    "2. Write out the formulas for the Extended Kalman Filter.\n",
    "3. Apply an Extended Kalman filter to the motion simulation above to track the location of the vehicle. Observations can be simulated by using previous simulation data as actual data, i.e. use this as the observed data ($z_k$). Parameters:\n",
    "   $$x_{0|0} = (0,0,0)$$\n",
    "   $$V = \\begin{pmatrix}.05 &  .02 & 0.01\\\\.02& .05& 0.01\\\\ 0.01& 0.01& .1\\end{pmatrix}$$\n",
    "   $$W = \\begin{pmatrix} .08& .02 \\\\.02&  .07\\end{pmatrix}$$\n",
    "   $$P_{0|0} = \\begin{pmatrix}2 &0& 0\\\\0 &1& 0\\\\0& 0& 0.5\\end{pmatrix}$$\n",
    "4. Output the $x, y$ locations on a 0.5 sec grid and compare in a plot.\n",
    "   \n",
    "   ??\n",
    "5. The covariance matrix $P$ gives the uncertainly ellipse for the location of the robot. Plot 5 ellipses along the path. This ellipse has major and minor axes given by the eigenvectors of $P$ and the axes lengths are given by the associated eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulas for the Extended Kalman Filter are as follows.\n",
    "\n",
    "$$\\vec x_k = \\begin{pmatrix}x_k \\\\ y_k \\\\ \\theta_k \\end{pmatrix}$$\n",
    "$$\\vec u_k = \\begin{pmatrix}\\omega_{1, k} \\\\ \\omega_{2, k} \\end{pmatrix}$$\n",
    "\n",
    "$$f(\\vec x_k, \\vec u_k) = \\begin{pmatrix}\n",
    "    x_k + \\frac{r \\Delta t}{2}(\\omega_{1, k} + \\omega_{2, k}) \\cos \\theta_k \\\\\n",
    "    y_k + \\frac{r \\Delta t}{2}(\\omega_{1, k} + \\omega_{2, k}) \\sin \\theta_k \\\\\n",
    "    \\theta_k + \\frac{}{2L}(\\omega_{1, k} - \\omega_{2, k})\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$h(\\vec x_k) = \\begin{pmatrix}x_k \\\\ y_k\\end{pmatrix}$$\n",
    "\n",
    "So then the Jacobians $F_k$ and $H_k$ are\n",
    "\n",
    "$$F_k = \\begin{pmatrix}\n",
    "    1 & 0 & -\\frac{r \\Delta t}{2}(\\omega_{1, k} + \\omega_{2, k}) \\sin \\theta_k \\\\\n",
    "    0 & 1 & \\frac{r \\Delta t}{2}(\\omega_{1, k} + \\omega_{2, k}) \\cos \\theta_k \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$H_k = \\begin{pmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then the Extended Kalman Filter formulas are\n",
    "\n",
    "* State prediction $$\\hat x_{x \\mid k-1} = f(\\hat x_{k - 1 \\mid k - 1}, \\vec u_k)$$\n",
    "* Prediction covariance $$P_{k \\mid k - 1} = F_k P_{k - 1 \\mid k - 1} F_k^T + V_k$$\n",
    "* Kalman gain $$K_k = P_{k \\mid k - 1} H_k^T {\\left(H_k P_{k \\mid k - 1} H_k^T + W_k\\right)}^{-1}$$\n",
    "* State update $$\\hat x_{k \\mid k} = \\hat x_{k \\mid k - 1} + K_k \\left(z_k - h(\\hat x_{k \\mid k - 1}\\right)$$\n",
    "* State update covariance $$P_{k \\mid k} = (I - K_k H_k)P_{k \\mid k - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeds(t):\n",
    "    \"\"\"Get the wheel speeds at the given time.\"\"\"\n",
    "    if 0 <= t <= 5:\n",
    "        return np.array([[3, 3]]).T\n",
    "    elif 5 < t <= 6:\n",
    "        return np.array([[1, -1]]).T\n",
    "    elif 6 < t <= 10:\n",
    "        return np.array([[3, 3]]).T\n",
    "    elif 10 < t <= 11:\n",
    "        return np.array([[-1, 1]]).T\n",
    "    elif 11 < t <= 16:\n",
    "        return np.array([[3, 3]]).T\n",
    "    else:\n",
    "        return np.array([[0, 0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([[0.05, 0.02, 0.01],\n",
    "              [0.02, 0.05, 0.01],\n",
    "              [0.01, 0.01, 0.1]])\n",
    "W = np.array([[0.08, 0.02],\n",
    "              [0.02, 0.07]])\n",
    "H = np.array([[1, 0, 0],\n",
    "              [0, 1, 0]])\n",
    "P0 = np.array([[2, 0, 0],\n",
    "               [0, 1, 0],\n",
    "               [0, 0, 0.5]])\n",
    "x0 = np.array([[0, 0, 0]]).T\n",
    "dt = 0.2\n",
    "r = 5\n",
    "# L is the half-wheelbase\n",
    "L = 12 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(state, control):\n",
    "    assert state.shape == (3, 1)\n",
    "    assert control.shape == (2, 1)\n",
    "    x, y, theta = state\n",
    "    w1, w2 = control\n",
    "    return np.array([\n",
    "        x + (r * dt / 2) * (w1 + w2) * np.cos(theta),\n",
    "        y + (r * dt / 2) * (w1 + w2) * np.sin(theta),\n",
    "        theta + (r * dt / (2 * L)) * (w1 - w2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(state, control):\n",
    "    assert state.shape == (3, 1)\n",
    "    assert control.shape == (2, 1)\n",
    "    x, y, theta = state\n",
    "    w1, w2 = control\n",
    "    return np.array([[1, 0, (-r * dt / 2) * (w1 + w2) * np.sin(theta)],\n",
    "                     [0, 1, (r * dt / 2) * (w1 + w2) * np.cos(theta)],\n",
    "                     [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(x0, t0):\n",
    "    \"\"\"Simulates the robot. Yields (state, observation, control) tuples.\"\"\"\n",
    "    while True:\n",
    "        control = speeds(t0)\n",
    "        x0 = f(x0, control) + np.random.multivariate_normal([0, 0, 0], V, 1).T\n",
    "        assert x0.shape == (3, 1)\n",
    "        observation = H @ x0 + np.random.multivariate_normal([0, 0], W, 1).T\n",
    "        assert observation.shape == (2, 1)\n",
    "        t0 += dt\n",
    "        yield x0, observation, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "sim = simulation(x0, 0)\n",
    "vals = [next(sim) for _ in np.arange(0, 16+dt, dt)]\n",
    "path, observations, _ = zip(*vals)\n",
    "x, y, theta = zip(*path)\n",
    "zx, zy = zip(*observations)\n",
    "\n",
    "plt.plot(x, y, label='Simulated Path')\n",
    "plt.plot(zx, zy, '.', label='Observed Path')\n",
    "\n",
    "plt.title('Differential Drive Robot Simulation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm confused by the wording of the problem. It states \"Observations can be simulated by using the previous simulation data as actual data.\" but then it states \"I.e., use this as the observed data.\" Shouldn't the observation add additional noise (using the $W$ covariance) because there's noise in both the process and the observation? The plot of the simulation above just isn't noisy enough for me to believe that that's all the noise I should add..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the path varies wildly between random runs. This is because we are injecting noise *during* the simulation, not after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x0, P0, t0):\n",
    "    \"\"\"Run the Extended Kalman Filter.\"\"\"\n",
    "    time = t0\n",
    "    estimate = x0\n",
    "    covariance = P0\n",
    "    for actual_state, observation, control in simulation(x0, t0):\n",
    "        assert actual_state.shape == (3, 1)\n",
    "        assert observation.shape == (2, 1)\n",
    "        assert control.shape == (2, 1)\n",
    "        prediction = f(estimate, control)\n",
    "        assert prediction.shape == (3, 1)\n",
    "        pred_covar = F(estimate, control) @ covariance @ F(estimate, control).T + V\n",
    "        assert pred_covar.shape == (3, 3)\n",
    "        kalman_gain = pred_covar @ H.T @ sp.linalg.inv(H @ pred_covar @ H.T + W)\n",
    "        assert kalman_gain.shape == (3, 2)\n",
    "        # Take the first two elements of the prediction because that's\n",
    "        # all we're observing\n",
    "        estimate = prediction + kalman_gain @ (observation - H @ prediction)\n",
    "        assert estimate.shape == (3, 1)\n",
    "        covariance = (np.eye(3) - kalman_gain @ H) @ pred_covar\n",
    "        \n",
    "        yield actual_state, observation, estimate, covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates its own simulation each time it's ran.\n",
    "ekf = filter(x0, P0, 0)\n",
    "vals = [next(ekf) for _ in np.arange(0, 16+dt, dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, observations, estimates, covariances = zip(*vals)\n",
    "\n",
    "x, y, theta = zip(*states)\n",
    "zx, zy = zip(*observations)\n",
    "xhat, yhat, that = zip(*estimates)\n",
    "\n",
    "# zip returns tuples. Convert to properly dimensioned numpy arrays for math.\n",
    "x = np.array(x).reshape(int(16 / dt + 1))\n",
    "y = np.array(y).reshape(int(16 / dt + 1))\n",
    "yhat = np.array(yhat).reshape(int(16 / dt + 1))\n",
    "xhat = np.array(xhat).reshape(int(16 / dt + 1))\n",
    "that = np.array(that).reshape(int(16 / dt + 1))\n",
    "\n",
    "# Ignore the cross variances because I'm not sure how to plot that...\n",
    "tstd = np.array([np.sqrt(cov[2, 2]) for cov in covariances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='Simulated Path')\n",
    "plt.plot(zx, zy, '.', label='Observed Path')\n",
    "plt.plot(xhat, yhat, label='EKF Path', color='r')\n",
    "\n",
    "plt.title('EKF Path Results')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 16+dt, dt), theta, label=r'$\\theta$')\n",
    "plt.plot(np.arange(0, 16+dt, dt), that, label=r'$\\hat\\theta$', color='r')\n",
    "plt.fill_between(np.arange(0, 16+dt, dt), that - tstd, that + tstd, label='1 std dev', color='r', alpha=0.2)\n",
    "\n",
    "plt.title(r'EKF $\\theta$ estimate')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel(r'$\\theta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='Simulated Path')\n",
    "\n",
    "for i in np.linspace(0, int(16 / dt), 5, dtype=int):\n",
    "    plt.plot(xhat[i], yhat[i], '.', color='r')\n",
    "    # Artificially expand the size of the ellipses because the plot\n",
    "    # is zoomed out too far.\n",
    "    plot_ellipse(covariances[i][0:2, 0:2], (xhat[i], yhat[i]), nstd=15, color='r', alpha=0.6)\n",
    "\n",
    "plt.title('EKF Error Ellipses')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The path given by the EKF is remarkably close to the actual simulated path. There are (at least) two reasons for this:\n",
    "    1. We are observing both $x$ and $y$.\n",
    "    2. The observation isn't very noisy.\n",
    "* The path is wildly different than the noise-free path. This is because we inject noise at every step of the simulation, not afterwards. This is expected.\n",
    "* The error ellipses at certain points along the path aren't very large at all. This is expected because we are observing both $x$ and $y$, so there will be only small amounts of variance for those variables. However, note the error bands on the plot of $\\theta$ versus $t$. This is where the noise manifests itself. We are not observing $\\theta$, so we have to rely on the physical model, which will amplify noise.\n",
    "  \n",
    "  If I were more ambitious, I would look into plotting 3D error ellipsoids so we could see that they are skinny in the $x$ and $y$ directions, bnut much fatter in the $\\theta$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same problem, but with different (more extreme) noise profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([[0.05, 0.02, 0.01],\n",
    "              [0.02, 0.05, 0.01],\n",
    "              [0.01, 0.01, 0.1]])\n",
    "W = np.array([[1.2, 0.3],\n",
    "              [0.3, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "sim = simulation(x0, 0)\n",
    "vals = [next(sim) for _ in np.arange(0, 16+dt, dt)]\n",
    "path, observations, _ = zip(*vals)\n",
    "x, y, theta = zip(*path)\n",
    "zx, zy = zip(*observations)\n",
    "\n",
    "plt.plot(x, y, label='Simulated Path')\n",
    "plt.plot(zx, zy, '.', label='Observed Path')\n",
    "\n",
    "plt.title('Differential Drive Robot Simulation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates its own simulation each time it's ran.\n",
    "ekf = filter(x0, P0, 0)\n",
    "vals = [next(ekf) for _ in np.arange(0, 16+dt, dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, observations, estimates, covariances = zip(*vals)\n",
    "\n",
    "x, y, theta = zip(*states)\n",
    "zx, zy = zip(*observations)\n",
    "xhat, yhat, that = zip(*estimates)\n",
    "\n",
    "# zip returns tuples. Convert to properly dimensioned numpy arrays for math.\n",
    "x = np.array(x).reshape(int(16 / dt + 1))\n",
    "y = np.array(y).reshape(int(16 / dt + 1))\n",
    "yhat = np.array(yhat).reshape(int(16 / dt + 1))\n",
    "xhat = np.array(xhat).reshape(int(16 / dt + 1))\n",
    "that = np.array(that).reshape(int(16 / dt + 1))\n",
    "\n",
    "# Ignore the cross variances because I'm not sure how to plot that...\n",
    "tstd = np.array([np.sqrt(cov[2, 2]) for cov in covariances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='Simulated Path')\n",
    "plt.plot(zx, zy, '.', label='Observed Path')\n",
    "plt.plot(xhat, yhat, label='EKF Path', color='r')\n",
    "\n",
    "plt.title('EKF Path Results')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 16+dt, dt), theta, label=r'$\\theta$')\n",
    "plt.plot(np.arange(0, 16+dt, dt), that, label=r'$\\hat\\theta$', color='r')\n",
    "plt.fill_between(np.arange(0, 16+dt, dt), that - tstd, that + tstd, label='1 std dev', color='r', alpha=0.2)\n",
    "\n",
    "plt.title(r'EKF $\\theta$ estimate')\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel(r'$\\theta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='Simulated Path')\n",
    "\n",
    "for i in np.linspace(0, int(16 / dt), 5, dtype=int):\n",
    "    plt.plot(xhat[i], yhat[i], '.', color='r')\n",
    "    # Artificially expand the size of the ellipses because the plot\n",
    "    # is zoomed out too far.\n",
    "    plot_ellipse(covariances[i][0:2, 0:2], (xhat[i], yhat[i]), nstd=15, color='r', alpha=0.6)\n",
    "\n",
    "plt.title('EKF Error Ellipses')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error ellipses are indeed larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Problem 18.1.1~~\n",
    "\n",
    "Deferred until after lectures are finished.\n",
    "\n",
    "Let the domain be the rectangle $0 \\leq x \\leq 15$ and $0 \\leq y \\leq 10$. Place the start position at $(0,5)$. Place the end position at $(15,5)$. Assume you have circular obstacles centered at $(6,4)$ with radius $2$ and at $(8,6)$ with radius $3$. Find a potential function which can navigate the robot from the start to the end position.\n",
    "\n",
    "1. Plot the resulting path in Python with obstacles included in the map.\n",
    "2. Compare to the Wavefront approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
