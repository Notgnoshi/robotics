{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \"6\"\n",
    "\n",
    "Austin Gill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config InlineBackend.figure_format = 'retina'\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "import sympy\n",
    "sympy.init_printing()\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "# Apparently, SNS stands for \"Samuel Norman Seaborn\", a fictional\n",
    "# character from The West Wing\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 18.1\n",
    "\n",
    "Let the domain be the rectangle $0 \\leq x \\leq 15$ and $0 \\leq y \\leq 10$. Place the start position at $(0,5)$. Place the end position at $(15,5)$. Assume you have circular obstacles centered at $(6,4)$ with radius $2$ and at $(8,6)$ with radius $3$. Find a potential function which can navigate the robot from the start to the end position.\n",
    "\n",
    "1. Plot the resulting path in Python with obstacles included in the map.\n",
    "2. Compare to the Wavefront approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $q_{goal}$ at $(15, 5)$, so we build\n",
    "\n",
    "$$U_a(q) = \\begin{cases}\n",
    "    \\frac{1}{2}\\gamma \\mathrm{d}^2(q, q_{goal}), &\\mathrm d(q, q_{goal}) \\leq \\mathrm d_{goal}^* \\\\\n",
    "    \\mathrm d_{goal}^* \\gamma \\mathrm d(q, q_{goal}) - \\frac{1}{2} \\gamma \\left(\\mathrm d_{goal}^*\\right)^2, &\\mathrm d(q, q_{goal}) > \\mathrm d_{goal}^* \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\mathrm d_{goal}^*$ is the distance from the goal at which we switch from a linear potential function to a quadratic, and $\\gamma$ is another configurable parameter with $\\mathrm d$ being the usual euclidean distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each of the two obstacles, we have a\n",
    "\n",
    "$$U_i = \\begin{cases}\n",
    "    \\frac{1}{2}\\eta \\left(\\frac{1}{\\mathrm D_i(q)} - \\frac{1}{Q^*}\\right), &\\mathrm D_i(q) \\leq Q^* \\\\\n",
    "    0, &\\mathrm D_i(q) > Q^* \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\mathrm D_i(q)$ is the distance to the $i$th obstacle and $Q^*$ is the cutoff distance after which the obstacle should not impact the potential field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular problem, we have\n",
    "\n",
    "$$U_a(x, y) = \\begin{cases}\n",
    "    \\gamma \\frac{1}{2} \\left((x - 15)^2 + (y - 5)^2\\right), &\\mathrm d(q, q_{goal}) \\leq \\mathrm d_{goal}^* \\\\\n",
    "    \\mathrm d_{goal}^* \\gamma \\sqrt{(x - 15)^2 + (y - 5)^2} - \\gamma \\frac{1}{2} \\left(\\mathrm d_{goal}^*\\right)^2, &\\text{ else}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$U_1 = \\begin{cases}\n",
    "    \\eta\\frac{1}{2}\\left(\\frac{1}{(x - 6)^2 + (y - 4)^2 - 4} - \\frac{1}{Q^*}\\right), &\\mathrm D_1(q) \\leq Q^* \\\\\n",
    "    0, &\\text{ else}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$U_2 = \\begin{cases}\n",
    "    \\eta\\frac{1}{2}\\left(\\frac{1}{(x - 8)^2 + (y - 6)^2 - 9} - \\frac{1}{Q^*}\\right), &\\mathrm D_2(q) \\leq Q^* \\\\\n",
    "    0, &\\text{ else}\n",
    "\\end{cases}$$\n",
    "\n",
    "and make wild ass guesses for $\\gamma$, $\\mathrm d_{goal}^*$, $Q^*$, and $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we plot the start point, goal, and obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(0, 5, '.', label='start')\n",
    "plt.plot(15, 5, '.', label='goal')\n",
    "ax = plt.gca()\n",
    "\n",
    "# Note that the height and width are *not* radii.\n",
    "ax.add_patch(Ellipse(xy=(8, 6), width=6, height=6, angle=0, color='r'))\n",
    "ax.add_patch(Ellipse(xy=(6, 4), width=4, height=4, angle=0, color='r'))\n",
    "\n",
    "plt.xlim(0-0.2, 16)\n",
    "plt.ylim(0-0.2, 10+0.2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Map')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the potential function that we generate. Note that I originally intended to use the piecewise functions from above, but ran into significant problems that I still don't understand. I also wish that the gradient near the obstacle was more gradual than it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1, 16, 0.1)\n",
    "y = np.arange(-1, 11, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "def U_a(x, y, gamma=1.0):\n",
    "    \"\"\"The attractive potential function.\"\"\"\n",
    "    d2 = (x - 15)**2 + (y - 5)**2\n",
    "    return 0.5 * gamma * d2\n",
    "\n",
    "def U_1(x, y, eta=2.0):\n",
    "    \"\"\"The repulsive potential function for the first obstacle.\"\"\"\n",
    "    d2 = (x - 6)**2 + (y - 4)**2 - 4\n",
    "    # Avoid division by zero, but only if not doing symbolic math.\n",
    "    if not isinstance(x, sympy.Symbol):\n",
    "        d2[d2 < 0.05] = 0.05\n",
    "    return 0.5 * eta / d2\n",
    "\n",
    "def U_2(x, y, eta=2.0):\n",
    "    \"\"\"The repulsive potential function for the second obstacle.\"\"\"\n",
    "    d2 = (x - 8)**2 + (y - 6)**2 - 9\n",
    "    # Avoid division by zero, but only if not doing symbolic math.\n",
    "    if not isinstance(x, sympy.Symbol):\n",
    "        d2[d2 < 0.05] = 0.05\n",
    "    return 0.5 * eta / d2\n",
    "\n",
    "Z = U_a(X, Y) + U_2(X, Y) + U_1(X, Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.view_init(azim=-90, elev=89)\n",
    "\n",
    "ax.plot_surface(X, Y, Z, linewidth=0.05)\n",
    "\n",
    "# ax.set_zlim(0, 10)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also plot the contours of the potential function. Due to the way I capped the repulsive potentials to avoid division by zero for a nice pretty plot, the top of each obstacle has the same contours as the original attractive potential function. So if we ever stepped inside the obstacle during gradient descent, we'd plot a path through the obstacle towards the goal.\n",
    "\n",
    "However, I've only capped the repulsive potentials for plotting purposes. Due to the symbolic nature of the gradient, the gradient descent algorithm will still see the infinite peaks along the edge of the obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X, Y, Z, levels=20)\n",
    "\n",
    "plt.plot(15, 5, '.', label='goal')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Note that the height and width are *not* radii.\n",
    "ax.add_patch(Ellipse(xy=(8, 6), width=6, height=6, angle=0, color='r'))\n",
    "ax.add_patch(Ellipse(xy=(6, 4), width=4, height=4, angle=0, color='r'))\n",
    "\n",
    "plt.xlim(-1, 16)\n",
    "plt.ylim(-1, 11)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Potential function contours')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.view_init(azim=-90, elev=89)\n",
    "\n",
    "ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False, cmap=sns.cubehelix_palette(dark=0.1, light=0.75, as_cmap=True))\n",
    "\n",
    "ax.contour(X, Y, Z, 20, zdir='z', offset=-50)\n",
    "ax.contour(X, Y, Z, zdir='x', offset=-5)\n",
    "ax.contour(X, Y, Z, 2, zdir='y', offset=15)\n",
    "\n",
    "ax.set_zlim(-50, 140)\n",
    "ax.set_xlim(-5, 16)\n",
    "ax.set_ylim(-1, 15)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "plt.title('Potential surface with contours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to perform gradient descent to find the path from $(0, 5)$ to $(15, 5)$ that avoids the obstacles. Unfortunately, I cannot find a way to get [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to return the path from the initial guess to the minima, so I have to compute the gradient and perform gradient descent myself.\n",
    "\n",
    "I guess that makes sense, because `scipy.optimize.minimize` is designed to *find* the minima, but in this case we know *exactly* where the minima is be design! What we're interested in is the *path* from the initial point to $q_{goal}$.\n",
    "\n",
    "So use `sympy` to symbolically create the potential function and compute its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sympy.symbols('x y')\n",
    "\n",
    "Potential = sympy.Matrix([U_a(x, y) + U_1(x, y) + U_2(x, y)])\n",
    "Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient = Potential.jacobian([x, y])\n",
    "Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert the `sympy` functions to `numpy` functions and test some points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = sympy.lambdify([x, y], Gradient, 'numpy')\n",
    "potential = sympy.lambdify([x, y], Potential, 'numpy')\n",
    "\n",
    "# Should point to (15, 5), which it almost does...\n",
    "print(gradient(15, 0))\n",
    "print(gradient(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have nice continuous forms of the potential and gradient, plot the vector field and weep. Unfortunately, it evenly spaces the vectors throughout the domain, which includes the obstacles. I do not feel adventurous enough to try to mask those values out. So this is what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabla = -gradient(X, Y)\n",
    "U, V = nabla[0]\n",
    "\n",
    "plt.title('Garbage Vector Field')\n",
    "plt.streamplot(X, Y, U, V, color=np.clip(potential(X, Y).reshape(X.shape), -100, 200), cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a little concerned that the symbolically computed gradients do not point straight towards the minima when evaluated at very simply points, but I'll slam on my \"I believe\" button for right now. Now perform the simplest gradient descent algorithm you've ever seen to record the path from $(0, 5)$ to $(15, 5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(gradient, x0, alpha, N):\n",
    "    \"\"\"Perform gradient descent on the given gradient.\"\"\"\n",
    "    path = [x0]\n",
    "    for i in range(N):\n",
    "        x0 = x0 - alpha * gradient(x0[0, 0], x0[0, 1])\n",
    "        path.append(x0)\n",
    "        \n",
    "    return np.array(path).reshape(N+1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path, alpha):\n",
    "    \"\"\"Plot the given path on the map.\"\"\"\n",
    "    plt.plot(*zip(*path), label='path')\n",
    "\n",
    "    plt.plot(0, 5, '.', label='start')\n",
    "    plt.plot(15, 5, '.', label='goal')\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Note that the height and width are *not* radii.\n",
    "    ax.add_patch(Ellipse(xy=(8, 6), width=6, height=6, angle=0, color='r'))\n",
    "    ax.add_patch(Ellipse(xy=(6, 4), width=4, height=4, angle=0, color='r'))\n",
    "\n",
    "    plt.xlim(-1, 16)\n",
    "    plt.ylim(-1, 11)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title(r'Gradient Descent with $\\alpha={}$'.format(alpha))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "N = 750\n",
    "x0 = np.array([[0, 5]])\n",
    "\n",
    "plot_path(descent(gradient, x0, alpha, N), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is *soo* much better than I ever expected! You know me though, I can't not screw with things, so let's do some experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [(0.02, 400), (0.025, 400), (0.03, 5000)]\n",
    "\n",
    "for alpha, N in configs:\n",
    "    plot_path(descent(gradient, x0, alpha, N), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the gradient descent algorithm I chose is *quite* sensitive to the step size I choose. This is expected, because if you step up the gradient around the obstacles, it pushes away from the obstacle harder than the goal attacts. So if we step too far, we'll step up the obstacle and then back away.\n",
    "\n",
    "In particular, note that the third doesn't even converge in fewer than 5000 iterations!!\n",
    "\n",
    "Now let's increase the repulsion factor $\\eta$ to $10$ and see if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Potential = sympy.Matrix([U_a(x, y, gamma=1.0) + U_1(x, y, eta=10.0) + U_2(x, y, eta=10.0)])\n",
    "Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient = Potential.jacobian([x, y])\n",
    "Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient2 = sympy.lambdify([x, y], Gradient, 'numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [(0.04, 400), (0.05, 150), (0.06, 100), (0.1, 100)]\n",
    "\n",
    "for alpha, N in configs:\n",
    "    plot_path(descent(gradient2, x0, alpha, N), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is the gradient descent more robust, it also pushes the path just slightly further from the obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "N = 750\n",
    "\n",
    "plot_path(descent(gradient, x0, alpha, N), alpha)\n",
    "plot_path(descent(gradient2, x0, alpha, N), alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
